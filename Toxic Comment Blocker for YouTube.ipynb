{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1040892163931-ehpeatruudvlqi9qtgii1pk8g29vj823.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyoutube.force-ssl&state=Pj7q0UxgyZdXk01fpbuJelkwEmVFlB&prompt=consent&access_type=offline\n",
      "Enter the authorization code: 4/uAGybHUKlQ8HKQrRED3Z98rpCBUJlOxVLpKflBKTmX48E30LxtdavZg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cdrgv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   comment_id                                   comment  label\n",
      "0  UgxIW33Vz2DpB9fKOQB4AaABAg                 fukc you guy like a sheet      1\n",
      "1  UgzyJ7HGJ2aIIX2CCC14AaABAg                 the quliti be like a crap      1\n",
      "2  UgymQDqlLOgkZ3-mhwZ4AaABAg  the qualiti of your video be like a shit      1\n",
      "3  UgyxT5PX5-jyl0D4UwB4AaABAg                                     idiot      1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "\n",
    "def Get_Next_Comment(video_Id,api_service_name,api_version,DEVELOPER_KEY,nextPageToken):\n",
    "    \n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_Id,\n",
    "        maxResults=\"100\",\n",
    "        pageToken = nextPageToken\n",
    "    )\n",
    "    response = request.execute()\n",
    "    response['nextPageToken']=response.get('nextPageToken',0)\n",
    "    if response['nextPageToken'] !=0:\n",
    "        for i in response['items']:\n",
    "            comment.append(i['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "            comment_id.append(i['id'])\n",
    "        Get_Next_Comment(video_Id,api_service_name,api_version,DEVELOPER_KEY,response['nextPageToken'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        for i in response['items']:\n",
    "            comment.append(i['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "            comment_id.append(i['id'])\n",
    "        return comment\n",
    "    \n",
    "def Request_Comment(youtube):\n",
    "   \n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_Id,\n",
    "        maxResults=\"100\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    response['nextPageToken']=response.get('nextPageToken',0)\n",
    "    \n",
    "    \n",
    "    for i in response['items']:\n",
    "        comment_id.append(i['id'])\n",
    "        comment.append(i['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "    \n",
    "    if response['nextPageToken'] !=0:\n",
    "      \n",
    "        Get_Next_Comment(video_Id,api_service_name,api_version,DEVELOPER_KEY,response['nextPageToken'])\n",
    "    \n",
    "def Comment_Cleaning():\n",
    "    punctuation_edit = string.punctuation.replace('\\'','') +\"0123456789\"\n",
    "    outtab = \"                                         \"\n",
    "    trantab = str.maketrans(punctuation_edit, outtab)\n",
    "\n",
    "    from stop_words import get_stop_words\n",
    "    stop_words = get_stop_words('english')\n",
    "    stop_words.append('')\n",
    "\n",
    "    for x in range(ord('b'), ord('z')+1):\n",
    "        stop_words.append(chr(x))\n",
    "\n",
    "\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    for i in range(len(comment)):\n",
    "        comment[i] = comment[i].lower().translate(trantab)\n",
    "        l = []\n",
    "        for word in comment[i].split():\n",
    "            l.append(stemmer.stem(lemmatiser.lemmatize(word,pos=\"v\")))\n",
    "        comment[i] = \" \".join(l)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def Comment_Classify():\n",
    "    count_vector = joblib.load(\"Joblib/vector.joblib\")\n",
    "    clf = joblib.load(\"Joblib/model.joblib\")\n",
    "    tf_test = count_vector.transform(comment)\n",
    "    predict =clf.predict(tf_test)\n",
    "    dataset = pd.DataFrame({'comment_id':comment_id,'comment':comment, 'label':predict})\n",
    "    print(dataset[dataset['label'] == 1])\n",
    "    toxic_list=dataset[dataset['label'] == 1]['comment_id'].tolist()\n",
    "    toxic_id=\"\"\n",
    "    for i in toxic_list:\n",
    "        if i != toxic_list[-1]:\n",
    "            toxic_id = toxic_id+i+','\n",
    "        else:\n",
    "            toxic_id = toxic_id+i\n",
    "        \n",
    "    return toxic_id\n",
    "    \n",
    "\n",
    "def Comment_Delete(youtube,toxic_id):\n",
    "\n",
    "    request2 = youtube.comments().setModerationStatus(\n",
    "        id=toxic_id,\n",
    "        moderationStatus=\"rejected\",\n",
    "        banAuthor=False\n",
    "    )\n",
    "    request2.execute()\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    \n",
    "    # Please put the path of your own credentail file here\n",
    "    client_secrets_file = \"client_secret.json\"\n",
    "    \n",
    "\n",
    "    # Get credentials and create an API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "    credentials = flow.run_console()\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "    \n",
    "    Request_Comment(youtube)\n",
    "    Comment_Cleaning()\n",
    "    toxic_id =Comment_Classify()\n",
    "    if toxic_id!=\"\":\n",
    "        Comment_Delete(youtube,toxic_id)\n",
    "    else:\n",
    "        print(\"There is no toxic comment from this video!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #Please put the video ID here\n",
    "    video_Id=\"\"  \n",
    "    \n",
    "    comment =[]\n",
    "    comment_id=[]    \n",
    "    main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
